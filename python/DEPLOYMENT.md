# AutoGen æœ¬åœ°éƒ¨ç½²ä¸é…ç½®æŒ‡å—

æœ¬æ–‡æ¡£æä¾›å®Œæ•´çš„ AutoGen æœ¬åœ°éƒ¨ç½²ã€æ¨¡å‹é…ç½®å’Œä½¿ç”¨æŒ‡å—ã€‚

---

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ¨¡å‹é…ç½®ä¸­å¿ƒ](#æ¨¡å‹é…ç½®ä¸­å¿ƒ)
- [å¿«é€Ÿå¼€å§‹ç¤ºä¾‹](#å¿«é€Ÿå¼€å§‹ç¤ºä¾‹)
- [å¤šæ¨¡å‹åä½œæ¨¡å¼](#å¤šæ¨¡å‹åä½œæ¨¡å¼)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… uv åŒ…ç®¡ç†å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
pip install uv

# è¿›å…¥ python ç›®å½•å¹¶å®‰è£…æ‰€æœ‰ä¾èµ–
cd python
uv sync --all-extras
source .venv/bin/activate  # Linux/Mac
# æˆ– .venv\Scripts\activate  # Windows
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

åœ¨ `python/` ç›®å½•ä¸‹åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# python/.env

# =============================================================================
# é˜¿é‡Œåƒé—®ï¼ˆDashScopeï¼‰äº‘ç«¯æ¨¡å‹é…ç½®
# =============================================================================
DASHSCOPE_API_KEY=your_dashscope_api_key_here
# DASHSCOPE_MODEL=qwen-plus                    # å¯é€‰ï¼Œé»˜è®¤ qwen-plus
# DASHSCOPE_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# =============================================================================
# Ollama æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆæ— éœ€ API Keyï¼‰
# =============================================================================
# OLLAMA_MODEL=qwen3:4b                        # å¯é€‰ï¼Œé»˜è®¤ qwen3:4b
# OLLAMA_BASE_URL=http://localhost:11434/v1   # å¯é€‰ï¼Œé»˜è®¤æœ¬åœ°åœ°å€
```

### 3. å¯åŠ¨ Ollama æœåŠ¡

```bash
# æ‹‰å–æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
ollama pull qwen3:4b

# Ollama é€šå¸¸ä¼šè‡ªåŠ¨åœ¨åå°è¿è¡Œ
# æ£€æŸ¥æ˜¯å¦è¿è¡Œï¼šè®¿é—® http://localhost:11434 åº”è¯¥è¿”å› "Ollama is running"
```

---

## ğŸ¯ æ¨¡å‹é…ç½®ä¸­å¿ƒ

### é…ç½®æ–‡ä»¶ä½ç½®

```
python/
â”œâ”€â”€ model_clients_registry.py    â† æ¨¡å‹æ³¨å†Œä¸­å¿ƒï¼ˆæ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼‰
â”œâ”€â”€ .env                          â† ç¯å¢ƒå˜é‡é…ç½®
â””â”€â”€ quickstart/
    â”œâ”€â”€ quickstart_qwen.py        â† å•æ¨¡å‹ç¤ºä¾‹ï¼ˆåƒé—®äº‘ç«¯ï¼‰
    â”œâ”€â”€ quickstart_ollama.py      â† å•æ¨¡å‹ç¤ºä¾‹ï¼ˆOllamaæœ¬åœ°ï¼‰
    â”œâ”€â”€ quickstart_multi.py       â† å¤šæ¨¡å‹åä½œç¤ºä¾‹
    â””â”€â”€ quickstart_selector_demo.py â† é€‰æ‹©å™¨æ¨¡å¼æµ‹è¯•
```

### å·²æ³¨å†Œçš„æ¨¡å‹æ¥å£

åœ¨ `model_clients_registry.py` ä¸­å·²é…ç½®ä»¥ä¸‹æ¨¡å‹ï¼š

| æ¨¡å‹åç§° | å‡½æ•° | ç±»å‹ | API Key | è¯´æ˜ |
|---------|------|------|---------|------|
| `qwen_dashscope` | `build_qwen_dashscope()` | äº‘ç«¯ | âœ… å¿…éœ€ | é˜¿é‡Œåƒé—®äº‘ç«¯æ¨¡å‹ï¼ŒåŠŸèƒ½å¼ºå¤§ |
| `ollama_qwen3` | `build_ollama_qwen3()` | æœ¬åœ° | âŒ ä¸éœ€è¦ | Ollamaæœ¬åœ°æ¨¡å‹ï¼Œå¿«é€Ÿå“åº” |

### å¦‚ä½•æ–°å¢æ¨¡å‹

ç¼–è¾‘ `model_clients_registry.py`ï¼š

```python
# 1. å®šä¹‰æ„å»ºå‡½æ•°
def build_your_model() -> OpenAIChatCompletionClient:
    """æ„å»ºä½ çš„æ¨¡å‹å®¢æˆ·ç«¯"""
    return OpenAIChatCompletionClient(
        model=os.getenv("YOUR_MODEL_NAME", "default-model"),
        api_key=_require_env("YOUR_API_KEY"),  # æˆ– "ollama" å¦‚æœæ˜¯æœ¬åœ°
        base_url=os.getenv("YOUR_BASE_URL", "https://api.example.com/v1"),
        model_info={
            "family": "your_model_family",
            "function_calling": True,    # æ˜¯å¦æ”¯æŒå‡½æ•°è°ƒç”¨
            "json_output": True,         # æ˜¯å¦æ”¯æŒJSONè¾“å‡º
            "structured_output": True,   # æ˜¯å¦æ”¯æŒç»“æ„åŒ–è¾“å‡º
            "vision": False,             # æ˜¯å¦æ”¯æŒè§†è§‰ç†è§£
        },
    )

# 2. æ³¨å†Œåˆ°æ¨¡å‹æ„é€ è¡¨
MODEL_BUILDERS: Dict[str, Callable[[], OpenAIChatCompletionClient]] = {
    "qwen_dashscope": build_qwen_dashscope,
    "ollama_qwen3": build_ollama_qwen3,
    "your_model": build_your_model,  # â† æ–°å¢è¿™ä¸€è¡Œ
}
```

### ä½¿ç”¨æ¨¡å‹

```python
from model_clients_registry import get_model

# è·å–æ¨¡å‹å®¢æˆ·ç«¯
client = get_model("qwen_dashscope")  # æˆ– "ollama_qwen3"

# ä½¿ç”¨æ¨¡å‹åˆ›å»º Agent
from autogen_agentchat.agents import AssistantAgent

agent = AssistantAgent(
    name="Assistant",
    model_client=client,
    system_message="ä½ çš„ç³»ç»Ÿæç¤ºè¯"
)
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šå•æ¨¡å‹å¯¹è¯ï¼ˆåƒé—®äº‘ç«¯ï¼‰

```bash
cd python/quickstart
python quickstart_qwen.py
```

**ä»£ç ç»“æ„**ï¼š
```python
from model_clients_registry import get_model

# è·å–åƒé—®æ¨¡å‹
client = get_model("qwen_dashscope")

# åˆ›å»º Agent
agent = AssistantAgent(
    name="Assistant",
    model_client=client,
    system_message="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
)

# è¿è¡Œå¯¹è¯
await Console(agent.run_stream(task="ä½ å¥½"))
```

### ç¤ºä¾‹ 2ï¼šå•æ¨¡å‹å¯¹è¯ï¼ˆOllamaæœ¬åœ°ï¼‰

```bash
cd python/quickstart
python quickstart_ollama.py
```

**ç‰¹ç‚¹**ï¼šæ— éœ€ç½‘ç»œï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œï¼Œé€Ÿåº¦å¿«

### ç¤ºä¾‹ 3ï¼šå¤šæ¨¡å‹åä½œï¼ˆé€‰æ‹©å™¨æ¨¡å¼ï¼‰

```bash
cd python/quickstart
python quickstart_multi.py
```

**æ ¸å¿ƒä»£ç **ï¼š
```python
from autogen_agentchat.teams import SelectorGroupChat

# åˆ›å»ºå¤šä¸ª Agent
qwen_agent = AssistantAgent(
    name="CloudExpert",
    model_client=get_model("qwen_dashscope"),
    system_message="æ“…é•¿æ·±åº¦æŠ€æœ¯åˆ†æ..."
)

ollama_agent = AssistantAgent(
    name="QuickHelper",
    model_client=get_model("ollama_qwen3"),
    system_message="æ“…é•¿å¿«é€Ÿå“åº”..."
)

# åˆ›å»ºé€‰æ‹©å™¨å›¢é˜Ÿ
team = SelectorGroupChat(
    participants=[qwen_agent, ollama_agent],
    model_client=get_model("qwen_dashscope"),  # é€‰æ‹©å™¨ä½¿ç”¨çš„æ¨¡å‹
    termination_condition=MaxMessageTermination(max_messages=10),
)

# è¿è¡Œä»»åŠ¡
await Console(team.run_stream(task="ä½ çš„é—®é¢˜"))
```

---

## ğŸ¤ å¤šæ¨¡å‹åä½œæ¨¡å¼

### 1. RoundRobinGroupChatï¼ˆè½®è¯¢æ¨¡å¼ï¼‰

**ç‰¹ç‚¹**ï¼šå›ºå®šé¡ºåºè½®æµå‘è¨€
**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤šä¸ªè§†è§’è½®æµè®¨è®ºçš„åœºæ™¯

```python
from autogen_agentchat.teams import RoundRobinGroupChat

team = RoundRobinGroupChat(
    participants=[agent1, agent2],
    termination_condition=MaxMessageTermination(max_messages=6),
)
```

### 2. SelectorGroupChatï¼ˆé€‰æ‹©å™¨æ¨¡å¼ï¼‰

**ç‰¹ç‚¹**ï¼šæ ¹æ®ä»»åŠ¡è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ Agent
**é€‚ç”¨åœºæ™¯**ï¼šä¸åŒ Agent æœ‰æ˜ç¡®åˆ†å·¥ï¼Œéœ€è¦æ™ºèƒ½è°ƒåº¦

```python
from autogen_agentchat.teams import SelectorGroupChat

team = SelectorGroupChat(
    participants=[cloud_expert, local_helper],
    model_client=selector_model,  # ç”¨äºå†³ç­–çš„æ¨¡å‹
    termination_condition=MaxMessageTermination(max_messages=10),
)
```

**ä¼˜åŒ–å»ºè®®**ï¼š
- ç»™æ¯ä¸ª Agent æ˜ç¡®çš„ `system_message`ï¼Œæè¿°å…¶æ“…é•¿é¢†åŸŸ
- é€‰æ‹©å™¨æ¨¡å‹ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚åƒé—®äº‘ç«¯ï¼‰
- è®¾è®¡æ˜ç¡®çš„ä»»åŠ¡ç±»å‹æ¥è§¦å‘ä¸åŒçš„ Agent

### 3. è‡ªå®šä¹‰å›¢é˜Ÿæ¨¡å¼

å¯ä»¥ç»§æ‰¿ `BaseGroupChat` å®ç°è‡ªå®šä¹‰è°ƒåº¦é€»è¾‘ã€‚

---

## ğŸ”§ é…ç½®å‚è€ƒ

### Agent é…ç½®

```python
agent = AssistantAgent(
    name="AgentName",              # Agent åç§°ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
    model_client=client,           # æ¨¡å‹å®¢æˆ·ç«¯
    system_message="...",          # ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰è§’è‰²å’Œèƒ½åŠ›ï¼‰
    description="...",             # Agent æè¿°ï¼ˆå¯é€‰ï¼‰
)
```

### ç»ˆæ­¢æ¡ä»¶é…ç½®

```python
from autogen_agentchat.conditions import (
    MaxMessageTermination,         # æœ€å¤§æ¶ˆæ¯æ•°
    TextMentionTermination,        # ç‰¹å®šæ–‡æœ¬å‡ºç°
    TimeoutTermination,            # è¶…æ—¶
)

# æœ€å¤§æ¶ˆæ¯æ•°ç»ˆæ­¢
termination = MaxMessageTermination(max_messages=10)

# æ–‡æœ¬è§¦å‘ç»ˆæ­¢
termination = TextMentionTermination(text="TERMINATE")

# å¤šæ¡ä»¶ç»„åˆ
from autogen_agentchat.conditions import AndTermination, OrTermination

termination = OrTermination([
    MaxMessageTermination(max_messages=20),
    TextMentionTermination(text="DONE"),
])
```

### æ¨¡å‹å‚æ•°é…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­å¯ä»¥é…ç½®çš„å‚æ•°ï¼š

```bash
# åƒé—®äº‘ç«¯
DASHSCOPE_API_KEY=sk-xxx           # å¿…éœ€
DASHSCOPE_MODEL=qwen-plus          # å¯é€‰æ¨¡å‹ï¼šqwen-plus, qwen-max, qwen-turbo
DASHSCOPE_BASE_URL=...             # å¯é€‰ï¼Œé»˜è®¤å®˜æ–¹åœ°å€

# Ollama æœ¬åœ°
OLLAMA_MODEL=qwen3:4b              # å¯é€‰æ¨¡å‹ï¼šqwen3:4b, llama2, mistral ç­‰
OLLAMA_BASE_URL=http://localhost:11434/v1  # å¯é€‰ï¼Œé»˜è®¤æœ¬åœ°åœ°å€
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šOllama ç«¯å£å ç”¨

**é”™è¯¯ä¿¡æ¯**ï¼š
```
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address...
```

**è§£å†³æ–¹æ¡ˆ**ï¼šè¯´æ˜ Ollama å·²ç»åœ¨è¿è¡Œï¼Œæ— éœ€é‡å¤å¯åŠ¨

**éªŒè¯**ï¼šè®¿é—® http://localhost:11434 åº”è¯¥è¿”å› "Ollama is running"

### é—®é¢˜ 2ï¼šç¼ºå°‘ç¯å¢ƒå˜é‡

**é”™è¯¯ä¿¡æ¯**ï¼š
```
MissingEnvVar: ç¼ºå°‘ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY...
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®è®¤ `python/.env` æ–‡ä»¶å­˜åœ¨
2. æ£€æŸ¥æ–‡ä»¶ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„é…ç½®é¡¹
3. é‡æ–°æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ

### é—®é¢˜ 3ï¼šæ¨¡å‹ä¸å­˜åœ¨

**é”™è¯¯ä¿¡æ¯**ï¼š
```
KeyError: æœªçŸ¥æ¨¡å‹ 'xxx'ï¼Œå¯ç”¨é€‰é¡¹: ['qwen_dashscope', 'ollama_qwen3']
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ `model_clients_registry.py` ä¸­çš„ `MODEL_BUILDERS`
- ç¡®è®¤æ¨¡å‹åç§°æ‹¼å†™æ­£ç¡®
- å¦‚éœ€æ–°æ¨¡å‹ï¼Œå‚è€ƒ"å¦‚ä½•æ–°å¢æ¨¡å‹"ç« èŠ‚

### é—®é¢˜ 4ï¼šOllama æ¨¡å‹æœªå®‰è£…

**é”™è¯¯ä¿¡æ¯**ï¼šè¿æ¥åˆ° Ollama ä½†è¿”å›æ¨¡å‹ä¸å­˜åœ¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
ollama list

# å®‰è£…éœ€è¦çš„æ¨¡å‹
ollama pull qwen3:4b
```

### é—®é¢˜ 5ï¼šè™šæ‹Ÿç¯å¢ƒé—®é¢˜

**ç—‡çŠ¶**ï¼šå¯¼å…¥æ¨¡å—å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é‡æ–°åŒæ­¥ç¯å¢ƒ
cd python
uv sync --all-extras
source .venv/bin/activate  # æˆ– .venv\Scripts\activate (Windows)
```

---

## ğŸ“š é«˜çº§é…ç½®

### æ—¥å¿—é…ç½®

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO)

# æŸ¥çœ‹è¯¦ç»†çš„ HTTP è¯·æ±‚æ—¥å¿—
logging.getLogger("autogen_ext.models.openai").setLevel(logging.DEBUG)
```

### ä»£ç†é…ç½®

å¦‚æœéœ€è¦é€šè¿‡ä»£ç†è®¿é—®äº‘ç«¯æ¨¡å‹ï¼š

```python
# åœ¨ model_clients_registry.py ä¸­æ·»åŠ 
import os

def build_qwen_dashscope() -> OpenAIChatCompletionClient:
    return OpenAIChatCompletionClient(
        model=os.getenv("DASHSCOPE_MODEL", "qwen-plus"),
        api_key=_require_env("DASHSCOPE_API_KEY"),
        base_url=os.getenv("DASHSCOPE_BASE_URL", "..."),
        # æ·»åŠ ä»£ç†é…ç½®
        http_client=httpx.Client(
            proxies=os.getenv("HTTP_PROXY"),
            timeout=30.0,
        ),
    )
```

### é‡è¯•ç­–ç•¥

```python
# åœ¨åˆ›å»º OpenAIChatCompletionClient æ—¶å¯ä»¥é…ç½®é‡è¯•
client = OpenAIChatCompletionClient(
    model="qwen-plus",
    api_key="...",
    base_url="...",
    max_retries=3,           # æœ€å¤§é‡è¯•æ¬¡æ•°
    timeout=60.0,            # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
)
```

---

## ğŸ“ è·å–å¸®åŠ©

- **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://microsoft.github.io/autogen/
- **GitHub Issues**ï¼šhttps://github.com/microsoft/autogen/issues
- **Discord ç¤¾åŒº**ï¼šhttps://discord.gg/pAbnFJrkgZ

---

## ğŸ”„ ç‰ˆæœ¬ä¿¡æ¯

- **AutoGen ç‰ˆæœ¬**ï¼š0.4.x
- **Python ç‰ˆæœ¬**ï¼š3.10+
- **æ–‡æ¡£æ›´æ–°æ—¥æœŸ**ï¼š2026-01-30

---

## âœ… æ£€æŸ¥æ¸…å•

éƒ¨ç½²å‰ç¡®è®¤ï¼š

- [ ] å·²å®‰è£… `uv` å¹¶åŒæ­¥ä¾èµ–
- [ ] å·²åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½® API Key
- [ ] Ollama æœåŠ¡æ­£å¸¸è¿è¡Œï¼ˆå¦‚éœ€ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
- [ ] å·²æ‹‰å–æ‰€éœ€çš„ Ollama æ¨¡å‹
- [ ] èƒ½å¤ŸæˆåŠŸå¯¼å…¥ `from model_clients_registry import get_model`
- [ ] è¿è¡Œ `quickstart_qwen.py` æˆ– `quickstart_ollama.py` éªŒè¯é…ç½®

---

**æç¤º**ï¼šå»ºè®®å°†æœ¬æ–‡æ¡£å’Œ `.env.example` æ–‡ä»¶ä¸€èµ·æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ï¼Œä½† `.env` æ–‡ä»¶åº”è¯¥åœ¨ `.gitignore` ä¸­æ’é™¤ã€‚
